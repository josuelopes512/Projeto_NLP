{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dObDfSqXtdrW"
      },
      "outputs": [],
      "source": [
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install nltk\n",
        "!pip install textract\n",
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!python -m spacy download pt_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXZF6b85tdrZ"
      },
      "source": [
        "# NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXCBEJtEtdrb"
      },
      "source": [
        "##### VISÃO GERAL\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed0QHux1tdrc"
      },
      "source": [
        "Uma empresa contratante deseja estabelecer termos de maior relevância em um documento\n",
        "específico. Neste caso, considere o histórico de exames, consultas e procedimentos realizados\n",
        "por um paciente. Um sistema deve ser desenvolvido para que o médico possa ter uma visão\n",
        "geral do histórico do paciente sem a necessidade de analisar documento por documento. Com\n",
        "base nesta importância, vamos desenvolver uma etapa deste sistema. Tokenizar um texto,\n",
        "realizar remoção de stopwords, aplicar o processo de lematização e fazer uma análise\n",
        "quantitativa e visual subjetiva deste."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeUqaw70tdrd"
      },
      "source": [
        "##### OBJETIVOS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0f6S4dMtdrd"
      },
      "source": [
        "1. Carregar o conjunto de documentos em PDF e armazená-los em alguma estrutura de dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIWut5Zktdre"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "\n",
        "pdf_files = glob('documentos\\\\*.pdf') # Carregando os arquivos pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwOBbVIatdrf"
      },
      "outputs": [],
      "source": [
        "import textract\n",
        "\n",
        "read_pdfs = [textract.process(i, encoding='utf-8').decode('utf-8') for i in pdf_files]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSEu5o7gtdrg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(data={\"textos\": read_pdfs})\n",
        "df.textos = df.textos.astype('string')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O3rSdIZtdrh"
      },
      "source": [
        "2. Realizar o pré-processamento destes ( tokenização e remoção de stop words, deixar todos os\n",
        "caracteres minúsculos...)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fr6Bo0Yitdri",
        "outputId": "09268fa5-b4e5-40a3-998f-9ace8478204e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>O câncer de pulmão é a doença maligna mais com...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   textos\n",
              "count                                                   3\n",
              "unique                                                  3\n",
              "top     O câncer de pulmão é a doença maligna mais com...\n",
              "freq                                                    1"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FH-3hrKtdrj"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer \n",
        "from nltk import word_tokenize\n",
        "from spacy.lang.pt.stop_words import STOP_WORDS\n",
        "\n",
        "\n",
        "def remove_stopwords(word):\n",
        "    stop_words = list(set(stopwords.words('portuguese') + list(STOP_WORDS)))\n",
        "    return \" \".join([i for i in word_tokenize(word) if i not in stop_words])\n",
        "\n",
        "\n",
        "def standardize_text(df, text_field):   \n",
        "    df_copy = df.copy()\n",
        "    df_copy[text_field] = df_copy[text_field].str.lower()\n",
        "    df_copy[text_field] = df_copy[text_field].apply(lambda x: remove_stopwords(x))\n",
        "    df_copy[text_field] = df_copy[text_field].map(lambda x: re.sub(r'http\\S+', '', x))   \n",
        "    df_copy[text_field] = df_copy[text_field].str.replace(r\"http\", \"\")    \n",
        "    df_copy[text_field] = df_copy[text_field].map(lambda x: re.sub(r'\"@\\S+', '', x))   \n",
        "    df_copy[text_field] = df_copy[text_field].map(lambda x: re.sub(r'\\d+', '', x))   \n",
        "    df_copy[text_field] = df_copy[text_field].str.replace(r\"@\", \"at\")    \n",
        "    df_copy[text_field] = df_copy[text_field].str.strip()\n",
        "    return df_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7-lYsrCtdrk",
        "outputId": "4a5ef584-87c7-4c12-daf5-63f9b4811785"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>câncer pulmão doença maligna comum mundo ; cas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>relativamente poucos dados brasil respeito cân...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dados exatos respeito procedimentos cirúrgicos...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              textos\n",
              "0  câncer pulmão doença maligna comum mundo ; cas...\n",
              "1  relativamente poucos dados brasil respeito cân...\n",
              "2  dados exatos respeito procedimentos cirúrgicos..."
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_questions = standardize_text(df, \"textos\") \n",
        "clean_questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGCCbIU6tdrk",
        "outputId": "50ba59fd-07bc-4ea7-c73c-856b83e01900"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textos</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>câncer pulmão doença maligna comum mundo ; cas...</td>\n",
              "      <td>[câncer, pulmão, doença, maligna, comum, mundo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>relativamente poucos dados brasil respeito cân...</td>\n",
              "      <td>[relativamente, poucos, dados, brasil, respeit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dados exatos respeito procedimentos cirúrgicos...</td>\n",
              "      <td>[dados, exatos, respeito, procedimentos, cirúr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              textos  \\\n",
              "0  câncer pulmão doença maligna comum mundo ; cas...   \n",
              "1  relativamente poucos dados brasil respeito cân...   \n",
              "2  dados exatos respeito procedimentos cirúrgicos...   \n",
              "\n",
              "                                              tokens  \n",
              "0  [câncer, pulmão, doença, maligna, comum, mundo...  \n",
              "1  [relativamente, poucos, dados, brasil, respeit...  \n",
              "2  [dados, exatos, respeito, procedimentos, cirúr...  "
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import RegexpTokenizer \n",
        "tokenizer = RegexpTokenizer(r'\\w+') \n",
        "clean_questions[\"tokens\"] = clean_questions[\"textos\"].apply(tokenizer.tokenize)\n",
        "clean_questions.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIZJXFWKtdrl",
        "outputId": "b61bb546-24ce-485e-caae-8538667ae6e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "668 Quantidade total de palavras, com um vocabulario de 355\n",
            "Tamanho máximo de uma sentença 298\n"
          ]
        }
      ],
      "source": [
        "#Inspecionando novamente os dados \n",
        "\n",
        "all_words = [word for tokens in clean_questions[\"tokens\"] for word in tokens]\n",
        "sentence_lengths = [len(tokens) for tokens in clean_questions[\"tokens\"]] \n",
        "VOCAB = sorted(list(set(all_words))) \n",
        "print(\"%s Quantidade total de palavras, com um vocabulario de %s\" % (len(all_words), len(VOCAB))) \n",
        "print(\"Tamanho máximo de uma sentença %s\" % max(sentence_lengths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0NOb_e8tdrl",
        "outputId": "33b3fa53-6562-492f-a238-b4d9bfa09383"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#Distribuição das sentenças por quantidade de palavras \n",
        "\n",
        "fig = plt.figure(figsize=(5, 5)) \n",
        "plt.xlabel('Tamanho da setença') \n",
        "plt.ylabel('Número de sentenças') \n",
        "plt.hist(sentence_lengths) \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtBF8pVGtdrm"
      },
      "source": [
        "3. Lematização com a Lib stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6oCXS_xtdrm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c16Gpa4tdrn"
      },
      "source": [
        "4. Implementar para determinar as seguintes informações dos resultados obtidos em 3:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgX_DUmotdrn"
      },
      "source": [
        "4. <br>  1. Term Frequency (TF):\n",
        "<br> 𝑇𝐹 = 𝑞𝑡𝑑 𝑑𝑒 𝑜𝑐𝑜𝑟𝑟ê𝑛𝑐𝑖𝑎 𝑑𝑜 𝑡𝑒𝑟𝑚𝑜 𝑒𝑚 𝑢𝑚 𝑡𝑒𝑥𝑡𝑜 / 𝑞𝑢𝑎𝑛𝑡𝑖𝑑𝑎𝑑𝑒 𝑡𝑜𝑡𝑎𝑙 𝑑𝑒 𝑝𝑎𝑙𝑎𝑣𝑟𝑎𝑠 𝑑𝑜 𝑡𝑒𝑥𝑡𝑜\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E13HqTMtdrn"
      },
      "source": [
        "4. <br>  2. Document Frequency (DF):\n",
        "<br> 𝐷𝐹 = 𝑞𝑡𝑑 𝑑𝑒 𝑜𝑐𝑜𝑟𝑟ê𝑛𝑐𝑖𝑎 𝑑𝑜 𝑡𝑒𝑟𝑚𝑜 𝑒𝑚 𝑢𝑚 𝑐𝑜𝑛𝑗𝑢𝑛𝑡𝑜 𝑑𝑒 𝑑𝑜𝑐𝑢𝑚𝑒𝑛𝑡𝑜𝑠\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpX8HHHAtdro"
      },
      "source": [
        "4. <br>  3. Inverse Document Frequency (IDF):\n",
        "<br> 𝐼𝐷𝐹 = 𝑙𝑜𝑔(𝑞𝑡𝑑 𝑑𝑒 𝑑𝑜𝑐𝑢𝑚𝑒𝑛𝑡𝑜𝑠 / (𝐷𝐹 + 1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8dGYxlctdro"
      },
      "source": [
        "4. <br>  4. TF-IDF:\n",
        "<br> 𝑇𝐹 − 𝐼𝐷𝐹 = 𝐼𝐷𝐹 * 𝑇𝐹\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMPzAoZHtdro"
      },
      "source": [
        "4. <br>  5. Lista de strings com proximidade até 2 dos 5 termos de maior TF-IDF. Essas strings\n",
        "devem ser acompanhadas de seu valor de TF. Exemplo: Suponha que a lista dos 5 termos de maior TF-IDF é [ casa, carro, comida, cachorro, gato]. Carro em um uma frase pode ter pneu e\n",
        "banco com as palavras mais próximas. Em outra parte do texto, carro pode ter volante e cinto,\n",
        "como as palavras mais próximas. Neste caso, para o termo carro, as strings [\n",
        "pneu,banco,volante,cinto] são as que devem ser armazenadas para análise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-MWpaTftdro"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP_Vh6VHtdrp"
      },
      "source": [
        "5. Gerar um arquivo csv que possui todas as palavras de todos os documentos na primeira coluna,\n",
        "em que cada linha é um token. Para cada token, informe nas colunas vizinhas as informações\n",
        "determinadas no objetivo 4.1 até 4.4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MTSCd_OVtdrp"
      },
      "outputs": [],
      "source": [
        "# Resolvendo a questão 5 utilizando dados sintéticos.\n",
        "# Dados\n",
        "word_array = [\"aloha\", \"gola\", \"camelo\", \"artigo\", \"nano\"]\n",
        "tf_array = [2,9,6,19,12]\n",
        "df_array = [10,20,15,50,42]\n",
        "idf_array = [5, 3, 6, 3, 9]\n",
        "tf_idf_array = [1.3, 5.6, 7.2, 3.2, 8.9]\n",
        "\n",
        "\n",
        "# Função\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def generate_token_dataframe(token_list: list,\n",
        "                             metrics_list: list,\n",
        "                             columns: list,\n",
        "                             to_csv: bool = False) -> pd.core.frame.DataFrame:\n",
        "    \"\"\"\n",
        "    Returns a pandas DataFrame class object and saves it to a .csv file if to_csv == True.\n",
        "    \"\"\"\n",
        " \n",
        "    token_dataframe = pd.DataFrame(list(zip(*metrics_list)),\n",
        "                                  index=token_list,\n",
        "                                  columns=columns)\n",
        "    if to_csv:\n",
        "      token_dataframe.to_csv(\"token_dataframe.csv\",index=True)\n",
        "    \n",
        "    return token_dataframe\n",
        "\n",
        "# Demonstração\n",
        "\n",
        "\n",
        "data_1 = generate_token_dataframe(token_list=word_array,\n",
        "                                  metrics_list=[tf_array, df_array,idf_array,tf_idf_array],\n",
        "                                  columns=[\"TF\",\"DF\", \"IDF\",\"TF-IDF\"],\n",
        "                                  to_csv=False)\n",
        "\n",
        "data_1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7m6Zd7mBtdrp"
      },
      "source": [
        "6. Gerar nuvem de palavras para análise visual tal como exemplo abaixo. Cada ponto central será\n",
        "um dos 5 termos de maior TF-IDF. As conexões são as palavras próximas obtidas em 4.5. O\n",
        "tamanho do círculo da palavra é baseado no TF dela. O maior círculo que conecta o termo\n",
        "central será normalizado para palavras de maior TF do conjunto.\n",
        "![MarineGEO circle logo](https://i.imgur.com/gNv4V7l.png \"MarineGEO logo\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9kYFDFMtdrp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPnlq3YAtdrp"
      },
      "source": [
        "**Tópicos de Auxílio**\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "Informações sobre as métricas utilizadas\n",
        "<br>\n",
        "https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\n",
        "<br>\n",
        "<br>\n",
        "Atividade determinação da nuvem de palavras\n",
        "<br>\n",
        "https://www.kaggle.com/code/arthurtok/ghastly-network-and-d3-js-force-directed-graphs/notebook\n",
        "<br>\n",
        "http://andrewtrick.com/stormlight_network.html"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "projeto_nlp.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "96068ab639a04010ce0ebf4d92aa5fa1d95945955db043eb093ed651d9ac7c11"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
